---
layout: post
title: Notes for Computer Graphics
categories: Computer-Graphics SFU-Course
description: Notes prepare for final exams
keywords: SFU, CMPT, 361
use_math: true
---
# Notes for final exam

## Input devices

* Physical
    * mouse, joystick, keyboard

* Logical
    * string, locator, choice

* Event-driven      


## Image formation

* Graphics system
    * Input
    * Processor (CPU)
    * Graphics processor (GPU)
    * Frame buffer

* **Raster** displays
    * A rectangular array of **pixels**
    * **Resolution**: number of pixels in an image, determines the details you can see

* **The frame buffer**
    * store the per-pixel information

* The GPU
    * special-purpose modules for graphics processing
    * high degree of **parallelism**

* CRT (Cathode-ray tube)
    * used frame buffer
    * screen need to be redrawn or **refreshed**  

* Flat-panel displays
    * E.g.
        * LCD (non-emissive)
        * LED and Plasma (emissive)
    * screen refresh no longer necessary

* High Dynamic Range Displays (**HDR**)

* Elements of image formation
    * Object
        * object space
    * Viewer
        * image space

* **First imaging algorithm**

* **Ray tracing**
    * Pros
        * follows the physics of optical flow
        * model **inter-surface reflections and refractions**
    * Cons
        * expensive
        * inadequate for modeling non-reflective objects

* **Z-buffer algorithm**
    * Per-polygon(per-object) operations
    * Simple and often accelerated with hardware - fast
    * No need to sort
    * A **visibility** algorithm and not for compute colors
    * Need other illumination model to compute colors (expensive!)

## Line Drawing

* **Graphics pipeline**
    * helps increase **throughput**
    * data processing in **subsequent blocks** can be done in **parallel**

* Vertex processor (per-vertex operations)
    * Coordinate transformations
    * Color computation at each **vertex**
    * object space to world space to camera space
    * modeling / viewing / projection transformation

* Clipper and primitive assembler
    * Model **a finite field of vision**
    * Remove objects outside **finite clipping volume**
    * on **primitive** basis, not vertices

* Rasterizer
    * Converts a primitive into a set of fragments
    * fragment stores per pixel info for its associated primitive

* Fragment processor (per-fragment operations)
    * Compute the color at a pixel

* What we want for line drawing
    * efficency!
    * pass both end points
    * uniform brightness
    * straight line

* DDA
    * floating-point addition still needed
    * large slope leads to discontinuity - fix using symmetry

* **Bresenham's algorithm**!!!
    * Idea: provide best-fit approximation to a true line by minimizing the error

## Clipping

* Scanline polygon rendering
    * **Odd-even test**
    * **Winding number test**
    * **Non-exterior rule**

* **Clipping**
    * why need this?
        * eye or camera have **a finite field of vision**  
        * less primitives to deal with - **efficiency**
        * OpenGL has **finite view volume**

* **Cohen-Sutherland algorithm**
    * Outcodes
    * Easily extended to 3D clipping

* **Sutherland-Hodgeman algorithm**
    * pipline approach (four sides independently)   

## 2D Transformation

* Affine space
    * A vector space + points
    * New: **Affine sum of points** / **convex sums**

* Euclidean space (matrix space)
    * A vector space + distance/norm

* **Convex hull**
    * set of all convex combinations of these points
    * the smallest convex object containing the set of points

* **Sided-ness test**
    * implicit line or plane equation, plug in the point coordinates and check the sign
    * cross product, check sign
    * dot product, check sign

* Basic transformations
    * **Rigid-body transform**
        * preserves length and angle
    * **Similarity transform**
        * preserve angle
    * **Affine (linear) transform**
        * preserve parallel lines
    * **Free-form deformation**

* Transformation
    * Translation
    * Scaling
    * Rotation
    * Shearing   

* **Homogeneous coordinates**
    * let translation be matrix too (uniform treatment)
    * w = 0 are **points at infinity**

* 2D lines
    * $ lp = 0 $

## 3D Transformation

* Implicit plan equation
    1. $ F(x, y, z) = ax + by + cz + d = 0 $
    2. Distance from a point to plane $dist=abs\(\frac{lp + d}{l}\)$

* Coordinate system
    * basis vectors
    * Frame
        * origin O + basis vectors

* $NQN^{-1}$
    * **Conjugate** of Q
    * Similar to Q
    * Q transformed by N

* **Adjugate** matrix
    * instead of inverses
    * $C^T$

* Transform of CS
    * $M_{y \leftarrow x}$
    * $CS_x = CS_yM_{y \leftarrow x}$  


## Viewing

* 3D Viewing -> clipping + projection + normalization

* **Perspective** vs. **parallel**
    * Perspective
        * Realistic
        * **Foreshortening**: size of object varies inversely with the distance from the COP
        * distances, angles, parallel lines not preserved
    * Parallel
        * Less realistic but useful for measurements
        * **Foreshorening uniform**
        * Parallel lines preserved

* **Projection normalization**
    * Normalize the parallel and perspective view volumes to a standard volume, called the **canonical view volume**
    * Preserves inside and outside (clipping), depth order (visibility).
    * Why need it?
        * simpler computation (e.g. clipping)
        * a unified graphic pipeline for both parallel and perspective projections

* **Projection matrix**
    * parallel
    * perspective

## Pipeline

* **Near plane**: used to (clip)eliminate objects which are too close to the camera (eye)

* When can clipping be done?
    * VCS: correct but expensive computations
    * CCS: homogeneous coordinates, correct but rather tricky
    * NDCS: most efficient but be aware of case where w < 0
        * depth information may get lost if w component < 0

* Note that
    * Negative w values occur only for points with z > 0
    * Depth order preserved in NDCS for those points with negative z in VCS

* So **clip out points behind COP in VCS and then project and clip in NDCS**

## Visibility

* Why visibility?
    * Realistic rendering
    * Efficiency
    * Less ambiguity: depth perception

* Classification
    * **Hidden surface removal**
    * **Visible surface determination**
    * Image spaces / object space algorithms
        * per pixel based(image precision) : ray tracing, z-buffering
            * rely on image resolution, can be computational expensive
        * per polygon or object based (object precision) : depth sort
            * independent of image resolution
            * more complex 3D interactions
            * can be expensive, need subdivision
        * or bybrid  

* **Efficient visibility techniques**
    * Projection normalization (done in hardware)
    * Coherence
        * Reuse computations made for one part of an object for nearby parts
            * without change or
            * with only **incremental updates**  
    * Back-face culling
        * In VCS, z < 0 does not work all the time, need plane equation or check sign of dot product
    * Bounding boxes
        * Avoid unnecessary clipping or intersection test
    * Spatial partitioning or spatial subdivision
        * Assign objects to spatially coherent groups in **preprocessing**
    * Hierarchy
        * Use information in the modeling hierarchy to restrict the need for intersection tests at lower levels, when children are part of parent in hierarchy

* Image-space algorithms
    * **z-buffer algorithm** – use of depth coherence
    * **Scanline algorithm** – use of scanline and depth coherences
    * **Warnock’s(area subdivision)algorithm** – use of area coherence and spatial subdivision

* Object-space algorithms
    * **Depth sort** - use of bounding or extent
    * **BSP algorithm** - use of partitioning

* Scanline algorithm
    * avoids unnecessary depth calculations in z-buffer (scanline coherence)
    * deal with sets of polygons instead of just one
    * less memory than z-buffer, but more complex algorithm

* Do area subdivision at **subpixel level**?
    * expensive but antaliasing

## Light

* light is a form of electromagnetic radiation

* 350 - 780 nm

* rods and cones

* HSV: hue (dominant wavelength) saturation lightness

* CIE XYZ colors: positive weights match all visible colors

* **Dominant wavelength**: spectral color that can be mixed with whit light to produce the desired color

## Local Illumination

* Motivation
    * **Visual realism** with surface appearance
    * provides 3D cues

* Features
    * More simplified physical model
    * Computes color of point **independently** through direct illumination
    * No **explicit** account for inter-object reflections

* (Global) **ambient light**
    * models **general brightness** of scene due to light sourse
    * light or eye location doesn't matter
    * rough model for inter-surface reflections due to all light sources
    * (Global): regardless of light sources

* **Phong reflection or local illumination model**: the color at a point on a surface due to one light source:  **I = ambient + diffuse + specular**

* **Lambertian reflection**
    * amount of light received on surface per unit surface area is $ Lcosθ $
    * amount of light seen by viewer per unit cross sectional area is $ Lk_dcosθ $

## Shading

* Illumination model: determine the color of a surface point by simulating light-material interaction

* Shading model: **colors the whole scene** after applying an illumination model at a set of point samples

* **Flat shading**
    * compute illumination once oer polygon and apply it to whole polygon or take color of first vertex
    * would be correct if
        * Light source infinity + viewer infinity + polygons represent actual surface, not approximation
    * if there are a very large number of vert small polygons, the faceting effects is less obvious - **spatial integration**

* **Mach band**
    * **Accentuation of intensity duscontinuity** along edges of a mesh

* **Gouraud/smooth shading**   
    * compute illumination at boundary and interpolate over the interior

* **Pointwise shading** e.g. Phong shading
    * compute illumination at evert pixel's point of the polygon  
    * **interpolates normals** for each pixel and apply LIM using normal
    * handles specular highlights much better but more expensive

* Problems
    * **polygon silhouette** : subdivision
    * **orientation dependence** : triangulation
    * **unrepresentative normals** : subdivision

## Global Illumination

* **More visual realism** with more faithful simulation of physics, but **much slower** than local illumination

* **incompatible with pipeline model** which shades each polygon independently

* About how to illuminate **the whole scene dependently**

* Two established algorithms
    * **Ray tracing**: discretize the image plane
        * View-dependent
        * Hybird medel, inculde LIM(direct illumination) before reflection and refraction
        * Ray-casting
            * Shadow rays
            * Reflections ((perfect mirror)reflect ray)
            * Transmission (refracted ray) (Snell's Law)
    * **Radiosity**: discretize the environment
    * Both are simplifications of the rendering equation

* Simplifying **rendering equation**
    * Ray tracing - view dependent
    * Radiosity - view independent
        * Discretize the scene into **pathches**, to be flat shaded
        * Each patch is assumed to be **perfectly diffuse**
        * Project patches onto image plane using computed colors
        * Light source using patch: **area light sources** - more realistic  

## Texture Mapping

* **Textures**
    * Textures are patterns

* **Texels**: the elements of the texture map - to distinguish from pixels

* **Area sampling**
    * Map the four corners of a screen pixel onto a surface in the scene
    * Use texture mapping to map the four surface points to points in texture map
    * use a weighted sum of texels covered by the quadrilateral in texture map to color pixel  

* **Point sampling**
    * Symply cast ray through the center of a pixel
    * Could be a severe case of **under-sampling** (aliasing)

* Two main issues
    * Finding the right mapping between the 2D texture and surface in 3D
        * One-to-one
        * Minimize distortion
        * Certain constraints
    * Finding the right color for each pixel
        * The key is to **reduce aliasing effects**    

* Persepective-correct interpolation
    * Interpolate in screen space, but instead of interpolating u and v, we interpolate u/z, v/z, and 1/z, at each pixel, we divide the interpolated values to get u and v.

* Sampling of texture maps
    * **Under-sampling** of texture map or **minification**: one pixel corresponds to a **texel region** （Fast averaging）
        * **Prefiltering**: faster computations for sum
            * **Mipmaps** (no aliasing, but may quite blurry)
            * **Summed area table** (SAT) (shaper and less blurry but more aliasing)
    * **Over-sampling** of texture map or **magnification**: one pixel corresponds to **portion of a texel** (Interpolation)
        * **nearest point sampling**: use texel closest to the point sample
        * **linear filtering**: ues average of group 2x2 texels
        * liner filtering reduces the **jaggies** over areas has over-sampling, but not areas has under-sampling, cause 2x2 filter is too small  

* Bump mapping
    * expensive cause need do lighting (Phong shading) at each point

## Curves

* Why curves and surfaces?
    * Natural
    * Compact representation
    * Well developed theory
    * Smoothness guaranteed

* Why polynomial?
    * Approximation power
    * Local control / piesewise
    * Easy for derivatives and intergrals
    * Compact representation
    * Efficient evaluation

* Fairness: global property / energy minimization
* Smoothness: local property / achieve by design

* Why cubic curves?
    * 0-2 too little flexibility
    * >3 not fairness  

* Parametric continuity vs Geometric continuity
